{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch 기본 사용법을 익혀보자  \n",
    "[basic tutorial](https://tutorials.pytorch.kr/beginner/pytorch_with_examples.html)  \n",
    "[advanced tutorial](https://greeksharifa.github.io/pytorch/2018/11/10/pytorch-usage-03-How-to-Use-PyTorch/)  \n",
    "[document](https://pytorch.org/docs/0.4.0/)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 29413356.707976185\n",
      "100 383.171623177115\n",
      "200 1.83676751468782\n",
      "300 0.01436896489429792\n",
      "400 0.00013603596254516758\n"
     ]
    }
   ],
   "source": [
    "\"\"\" numpy 만 사용하여 구현 \"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = np.random.randn(D_in, H)\n",
    "w2 = np.random.randn(H, D_out)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "\n",
    "    if t%100==0: print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h)\n",
    "\n",
    "    # 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34701728.0\n",
      "100 401.8963928222656\n",
      "200 2.2027335166931152\n",
      "300 0.02028506249189377\n",
      "400 0.00040454656118527055\n"
     ]
    }
   ],
   "source": [
    "\"\"\" pytorch module을 사용 \"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 무작위의 입력과 출력 데이터를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 무작위로 가중치를 초기화합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 예측값 y를 계산합니다.\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "\n",
    "    # 손실(loss)을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t%100==0: print(t, loss)\n",
    "\n",
    "    # 손실에 따른 w1, w2의 변화도를 계산하고 역전파합니다.\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "\n",
    "    # 경사하강법(gradient descent)를 사용하여 가중치를 갱신합니다.\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 34489028.0\n",
      "100 491.3359680175781\n",
      "200 3.5160603523254395\n",
      "300 0.04569462686777115\n",
      "400 0.0009537810110487044\n"
     ]
    }
   ],
   "source": [
    "\"\"\" backword함수를 사용하여 grad값을 구한다.\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=False로 설정하여 역전파 중에 이 Tensor들에 대한 변화도를 계산할\n",
    "# 필요가 없음을 나타냅니다. (requres_grad의 기본값이 False이므로 아래 코드에는\n",
    "# 이를 반영하지 않았습니다.)\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "# requires_grad=True로 설정하여 역전파 중에 이 Tensor들에 대한\n",
    "# 변화도를 계산할 필요가 있음을 나타냅니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산합니다. 이는 Tensor를\n",
    "    # 사용한 순전파 단계와 완전히 동일하지만, 역전파 단계를 별도로 구현하지 않아도\n",
    "    # 되므로 중간값들에 대한 참조(reference)를 갖고 있을 필요가 없습니다.\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "\n",
    "    # Tensor 연산을 사용하여 손실을 계산하고 출력합니다.\n",
    "    # loss는 (1,) 형태의 Tensor이며, loss.item()은 loss의 스칼라 값입니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t%100==0: print(t, loss.item())\n",
    "\n",
    "    # autograd를 사용하여 역전파 단계를 계산합니다. 이는 requires_grad=True를\n",
    "    # 갖는 모든 Tensor에 대해 손실의 변화도를 계산합니다. 이후 w1.grad와 w2.grad는\n",
    "    # w1과 w2 각각에 대한 손실의 변화도를 갖는 Tensor가 됩니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 수동으로 갱신합니다.\n",
    "    # torch.no_grad()로 감싸는 이유는 가중치들이 requires_grad=True이지만\n",
    "    # autograd에서는 이를 추적할 필요가 없기 때문입니다.\n",
    "    # 다른 방법은 weight.data 및 weight.grad.data를 조작하는 방법입니다.\n",
    "    # tensor.data가 tensor의 저장공간을 공유하기는 하지만, 이력을\n",
    "    # 추적하지 않는다는 것을 기억하십시오.\n",
    "    # 또한, 이를 위해 torch.optim.SGD 를 사용할 수도 있습니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 27294050.0\n",
      "100 453.8661804199219\n",
      "200 2.048283815383911\n",
      "300 0.01708075776696205\n",
      "400 0.00040155983879230917\n"
     ]
    }
   ],
   "source": [
    "\"\"\" 수동으로 optimize 연산을 통한 parameter들을 update하는 방식\"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    torch.autograd.Function을 상속받아 사용자 정의 autograd Function을 구현하고,\n",
    "    Tensor 연산을 하는 순전파와 역전파 단계를 구현하겠습니다.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        순전파 단계에서는 입력을 갖는 Tensor를 받아 출력을 갖는 Tensor를 반환합니다.\n",
    "        ctx는 컨텍스트 객체(context object)로 역전파 연산을 위한 정보 저장에\n",
    "        사용합니다. ctx.save_for_backward method를 사용하여 역전파 단계에서 사용할 어떠한\n",
    "        객체도 저장(cache)해 둘 수 있습니다.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        역전파 단계에서는 출력에 대한 손실의 변화도를 갖는 Tensor를 받고, 입력에\n",
    "        대한 손실의 변화도를 계산합니다.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # GPU에서 실행하려면 이 주석을 제거하세요.\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "# 가중치를 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(500):\n",
    "    # 사용자 정의 Function을 적용하기 위해 Function.apply 메소드를 사용합니다.\n",
    "    # 여기에 'relu'라는 이름을 붙였습니다.\n",
    "    relu = MyReLU.apply\n",
    "\n",
    "    # 순전파 단계: Tensor 연산을 사용하여 예상되는 y 값을 계산합니다;\n",
    "    # 사용자 정의 autograd 연산을 사용하여 ReLU를 계산합니다.\n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t%100==0: print(t, loss.item())\n",
    "\n",
    "    # autograde를 사용하여 역전파 단계를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 갱신합니다.\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 수동으로 변화도를 0으로 만듭니다.\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n",
      "0 665.529052734375\n",
      "100 43.233463287353516\n",
      "200 0.5490105152130127\n",
      "300 0.01082281582057476\n",
      "400 0.00019142901874147356\n"
     ]
    }
   ],
   "source": [
    "\"\"\" optim 사용하여 parameter update\"\"\"\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# nn 패키지를 사용하여 모델과 손실 함수를 정의합니다.\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# GPU를 사용하고 싶은경우 \n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    model = model.cuda()\n",
    "    print('cuda is available')\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(size_average=False, reduce=True)\n",
    "\n",
    "# optim 패키지를 사용하여 모델의 가중치를 갱신할 Optimizer를 정의합니다.\n",
    "# 여기서는 Adam을 사용하겠습니다; optim 패키지는 다른 다양한 최적화 알고리즘을\n",
    "# 포함하고 있습니다. Adam 생성자의 첫번째 인자는 어떤 Tensor가 갱신되어야 하는지\n",
    "# 알려줍니다.\n",
    "learning_rate = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t%100==0: print(t, loss.item())\n",
    "    \n",
    "    # 역전파 단계 전에, Optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인)\n",
    "    # 갱신할 변수들에 대한 모든 변화도를 0으로 만듭니다. 이렇게 하는 이유는\n",
    "    # 기본적으로 .backward()를 호출할 때마다 변화도가 버퍼(buffer)에 (덮어쓰지 않고)\n",
    "    # 누적되기 때문입니다. 더 자세한 내용은 torch.autograd.backward에 대한 문서를\n",
    "    # 참조하세요.\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 매개변수에 대한 손실의 변화도를 계산합니다.\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimizer의 step 함수를 호출하면 매개변수가 갱신됩니다.\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda is available\n",
      "0 712.8546142578125\n",
      "100 1.9003677368164062\n",
      "200 0.013929614797234535\n",
      "300 0.00018935385742224753\n",
      "400 3.710154032887658e-06\n"
     ]
    }
   ],
   "source": [
    "\"\"\" user define model사용 \"\"\"\n",
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        생성자에서 2개의 nn.Linear 모듈을 생성하고, 멤버 변수로 지정합니다.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 함수에서는 입력 데이터의 Tensor를 받고 출력 데이터의 Tensor를\n",
    "        반환해야 합니다. Tensor 상의 임의의 연산자뿐만 아니라 생성자에서 정의한\n",
    "        Module도 사용할 수 있습니다.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred\n",
    "\n",
    "\n",
    "# N은 배치 크기이며, D_in은 입력의 차원입니다;\n",
    "# H는 은닉층의 차원이며, D_out은 출력 차원입니다.\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "# 입력과 출력을 저장하기 위해 무작위 값을 갖는 Tensor를 생성합니다.\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "# 앞에서 정의한 클래스를 생성하여 모델을 구성합니다.\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# GPU를 사용하고 싶은경우 \n",
    "if torch.cuda.is_available():\n",
    "    x = x.cuda()\n",
    "    y = y.cuda()\n",
    "    model = model.cuda()\n",
    "    print('cuda is available')\n",
    "\n",
    "# 손실 함수와 Optimizer를 만듭니다. SGD 생성자에 model.parameters()를 호출하면\n",
    "# 모델의 멤버인 2개의 nn.Linear 모듈의 학습 가능한 매개변수들이 포함됩니다.\n",
    "criterion = torch.nn.MSELoss(size_average=False, reduce=True)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "for t in range(500):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예상되는 y 값을 계산합니다.\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t%100==0: print(t, loss.item())\n",
    "\n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신합니다.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
